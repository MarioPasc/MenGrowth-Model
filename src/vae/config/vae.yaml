data:
  root_dir: "/media/mpascual/PortableSSD/Meningiomas/BraTS/BraTS_Men_Train"
  modalities: ["t1c", "t1n", "t2f", "t2w"]
  roi_size: [128, 128, 128]
  spacing: [1.875, 1.875, 1.875]
  orientation: "RAS"
  batch_size: 16
  num_workers: 0
  cache_rate: 1.0
  val_split: 0.1
  persistent_cache_subdir: "cache"

model:
  z_dim: 128
  input_channels: 4
  base_filters: 32
  use_sbd: false  # Baseline VAE uses standard transposed-conv decoder
  norm: "GROUP"   # Normalization type (now properly used via model_factory)
  # SBD config keys (included for compatibility if use_sbd is changed to true)
  sbd_grid_size: [8, 8, 8]
  sbd_upsample_mode: "resize_conv"

  # Architecture customizations
  architecture:
    pre_activation: false             # Use pre-activation ResNet blocks (NORM->ACT->CONV)
    blocks_per_layer: [2, 2, 2, 2]    # Number of blocks in each of the 4 encoder stages
    upsample_mode: "resize_conv"      # Decoder upsampling: "resize_conv" or "deconv"

train:
  seed: 42
  max_epochs: 200
  lr: 1e-4

  # === KL Regularization - Posterior Collapse Mitigation ===
  # Default strategy: Cyclical Annealing + Free Bits (recommended)

  # Beta annealing (controls KL weight schedule)
  kl_beta: 1.0
  kl_annealing_epochs: 200  # For cyclical: 4 cycles × 40 epochs/cycle
  kl_annealing_type: "cyclical"  # Options: "linear", "cyclical"
  kl_annealing_cycles: 4
  kl_annealing_ratio: 0.5  # Anneal for first 50% of each cycle

  # Free bits (per-dimension KL threshold for posterior collapse prevention)
  # CRITICAL: With reduction="mean", free bits floor >> MSE_mean (~1).
  # This ensures the model MUST encode information to beat the floor.
  # Without free bits, KL would collapse to near-zero (posterior = prior).
  kl_free_bits: 0.1                # nats per dimension (0.0 = disabled)
                                   # With batch_mean mode and z_dim=128: floor ≈ 128 × 0.1 = 12.8 nats
                                   # Tune in range 0.05-0.2 based on KL/recon balance
  kl_free_bits_mode: "batch_mean"  # "per_sample" or "batch_mean"
                                   # batch_mean: weaker, better for small batches (B=2)
                                   # per_sample: stronger, uniform floor across samples

  # Capacity control (alternative to beta annealing - disabled by default)
  # kl_target_capacity: 20.0  # Uncomment to enable capacity control
  kl_capacity_anneal_epochs: 200

  # === Alternative Configurations (commented examples) ===
  #
  # Linear annealing only (original behavior):
  #   kl_annealing_type: "linear"
  #   kl_annealing_epochs: 40
  #   kl_free_bits: 0.0
  #
  # Capacity control only (replaces beta weighting):
  #   kl_target_capacity: 20.0
  #   kl_capacity_anneal_epochs: 100
  #   kl_free_bits: 0.0

  # Execution settings
  precision: "16-mixed" # or "32-true"
  accelerator: "gpu"
  devices: 1

  # === Loss Reduction Strategy ===
  # We use "mean" reduction for MSE but do NOT scale KL by 1/N.
  # This creates apparent imbalance (KL >> recon in magnitude) but is correct:
  # - Sum reduction would give MSE ~16M for 128³ volumes, causing FP16 overflow
  # - Mean reduction gives MSE ~1, numerically stable for FP16
  # - Keeping KL unscaled ensures free bits floor (25.6 nats) >> MSE (~1)
  # - The imbalance is handled by cyclical annealing and free bits floor
  # See tests/test_loss_reduction_analysis.py for detailed analysis.
  loss_reduction: "mean"

  # Numerical stability settings for FP16 mixed precision
  gradient_clip_val: 1.0  # Clip gradient L2 norm to prevent FP16 overflow
  gradient_clip_algorithm: "norm"  # Gradient clipping algorithm: "norm" or "value"

logging:
  save_dir: "experiments/runs"
  recon_every_n_epochs: 5
  num_recon_samples: 2
  min_logs_per_epoch: 3  # Minimum console/CSV log entries per training epoch

  # Latent diagnostics (lightweight disentanglement metrics)
  latent_diag_every_n_epochs: 10
  latent_diag_num_samples: 32
  latent_diag_shift_vox: 5
  latent_diag_csv_name: "latent_diag/metrics.csv"
  latent_diag_ids_name: "latent_diag/ids.txt"

  # Active Units (AU) callback - canonical latent activity metric
  au_dense_until: 15        # Dense logging (every epoch) for epochs 0..15
  au_sparse_interval: 5     # Sparse logging (every 5 epochs) after epoch 15
  au_subset_fraction: 0.33  # Use 33% of validation dataset for AU computation
  au_batch_size: 64         # Batch size for AU computation (larger = more efficient)
  eps_au: 0.01              # Variance threshold (nats) for active dimension counting
  au_subset_seed: 42        # RNG seed for reproducible subset selection

  # Tidy CSV logging (one row per epoch, no NaN fragmentation)
  tidy_dir: "logs/tidy"
  step_log_stride: 100  # Log every N steps for step CSV (50-200 recommended for long runs)

  # Segmentation label mapping (configurable for different datasets)
  seg_labels:
    ncr: 1  # Necrotic core
    ed: 2   # Edema
    et: 3   # Enhancing tumor

  # Gradient norm logging (optimization stability monitoring)
  log_grad_norm: true
