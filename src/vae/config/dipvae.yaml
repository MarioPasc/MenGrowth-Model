data:
  root_dir: "/media/mpascual/PortableSSD/Meningiomas/BraTS/BraTS_Men_Train"
  modalities: ["t1c", "t1n", "t2f", "t2w"]
  roi_size: [128, 128, 128]
  spacing: [1.875, 1.875, 1.875]
  orientation: "RAS"
  batch_size: 16
  num_workers: 0
  cache_rate: 1.0
  val_split: 0.1
  persistent_cache_subdir: "cache"

model:
  variant: "dipvae"
  z_dim: 128
  input_channels: 4

  # Architectural changes:
  base_filters: 32 
  norm: "GROUP"
  encoder_depth: 4
  decoder_depth: 4

  

  # Spatial Broadcast Decoder (SBD) 
  use_sbd: true                     # RECOMMENDED: false = standard decoder, true = SBD
                                     # Start with false to verify encoder learns informative latents
                                     # If collapse persists, architectural changes won't help
  # SBD-specific parameters (only used if use_sbd=true)
  sbd_grid_size: [8, 8, 8]
  sbd_upsample_mode: "resize_conv"  # "resize_conv" prevents checkerboard artifacts



loss:
  lambda_od: 10.0                      # Off-diagonal covariance penalty (DIP-VAE-II)
  lambda_d: 5.0                        # Diagonal covariance penalty (DIP-VAE-II)
  lambda_start_epoch: 20               # RECOMMENDED: Pre-train VAE for 20 epochs before DIP
                                       # Allows encoder to learn informative latents first
  lambda_cov_annealing_epochs: 40      # Linear warmup after start_epoch (prevents optimizer shock)
  compute_in_fp32: true
  reduction: "mean"                    # "mean" for numerical stability with large 3D volumes
  # Multi-GPU DDP: all-gather μ,logvar across ranks for larger effective covariance batch
  # Requires drop_last=True for equal batch sizes (enforced in datasets.py when devices > 1)
  use_ddp_gather: true                 # Enable all-gather for covariance when DDP active

train:
  seed: 42
  max_epochs: 200
  lr: 1e-4
  precision: "16-mixed"
  gradient_checkpointing: true
  accelerator: "gpu"
  devices: 1                           # If devices > 1, validation uses drop_last=True for DDP safety

  # Numerical stability settings
  loss_reduction: "mean"
  gradient_clip_val: 5.0               # CHANGED: 0.0 -> 5.0 (enable clipping)
  gradient_clip_algorithm: "norm"

  # Posterior variance floor (prevents numerical underflow)
  posterior_logvar_min: -6.0           # NEW: exp(-6) ≈ 0.0025 variance

  # === KL Regularization - Posterior Collapse Mitigation ===
  # Default strategy: Cyclical Annealing + Free Bits (recommended)

  # Beta annealing (controls KL weight schedule)
  kl_beta: 1.0
  kl_annealing_epochs: 160  # For cyclical: 4 cycles × 40 epochs/cycle
  kl_annealing_type: "cyclical"  # Options: "linear", "cyclical"
  kl_annealing_cycles: 4
  kl_annealing_ratio: 0.5  # Anneal for first 50% of each cycle

  # Free bits (per-dimension KL threshold for posterior collapse mitigation)
  kl_free_bits: 0.01                    # RECOMMENDED: 0.2 nats/dim for stronger collapse prevention
                                       # With batch_mean mode and z_dim=128: floor ≈ 128 × 0.2 = 25.6 nats
                                       # Start with 0.2, reduce to 0.1 if KL too high
  kl_free_bits_mode: "batch_mean"      # "per_sample" or "batch_mean"
                                       # batch_mean: weaker, better for small batches (B=8)
                                       # per_sample: stronger, uniform floor across samples

  # Collapse diagnostics (deterministic recon, z=0 ablation, μ variance)
  log_collapse_diagnostics: true       # Log diag/recon_mu_mse, diag/recon_z0_mse, etc.

logging:
  save_dir: "experiments/runs"

  # Logger configuration
  log_every_n_steps: 50  # How often to log step metrics

  logger:
    type: "wandb"  # "wandb" or "csv" (for backward compat)
    wandb:
      project: "mengrowth-dipvae"
      entity: "mario-pg02-icai"  # Set to your wandb username/team (or use WANDB_ENTITY env var)
      name: null  # Auto-generated: "{experiment}_{timestamp}"
      tags: ["dipvae", "meningioma", "disentanglement", "exp2", "sbd"]
      notes: "Training DIP-VAE with SBD on cluster - Covariance regularization for disentanglement"
      offline: true  # CRITICAL for cluster nodes without internet
      save_code: true  # Save code snapshot to wandb
      log_model: false  # We handle checkpoints manually

  # Checkpointing
  checkpointing:
    save_top_k: 3
    monitor: "val_epoch/loss"
    mode: "min"
    save_last: true
    every_n_epochs: 1
    filename: "dipvae-{epoch:04d}"  # Note: {val_loss} removed (key is val_epoch/loss)

  # Visual logging
  visual:
    log_reconstructions: true
    recon_every_n_epochs: 5
    num_recon_samples: 2
    log_dashboard: true
    dashboard_every_n_epochs: 10
    log_latent_viz: true
    latent_viz_every_n_epochs: 20
    latent_viz_n_samples: 512

  # Legacy settings (keep for backward compat)
  min_logs_per_epoch: 3                # Minimum console/CSV log entries per training epoch

  # Latent diagnostics (lightweight disentanglement metrics)
  latent_diag_every_n_epochs: 10
  latent_diag_num_samples: 128         # Increased from 32 for better covariance estimation
                                       # 128-512 recommended for z_dim=128
  latent_diag_shift_vox: 5
  latent_diag_csv_name: "latent_diag/metrics.csv"
  latent_diag_ids_name: "latent_diag/ids.txt"

  # Active Units (AU) callback - canonical latent activity metric
  au_dense_until: 30                   # Dense logging (every epoch) for epochs 0..30
  au_sparse_interval: 5                # Sparse logging (every 5 epochs) after epoch 30
  au_subset_fraction: 0.66             # Use 66% of validation dataset for AU computation
  au_batch_size: 64                    # Batch size for AU computation (larger = more efficient)
  eps_au: 0.01                         # Variance threshold (nats) for active dimension counting
  au_subset_seed: 42                   # RNG seed for reproducible subset selection

  # Tidy CSV logging (one row per epoch, no NaN fragmentation)
  tidy_dir: "logs/tidy"
  step_log_stride: 50                 # Log every N steps for step CSV (50-200 recommended for long runs)

  # Segmentation label mapping (configurable for different datasets)
  seg_labels:
    ncr: 1  # Necrotic core
    ed: 2   # Edema
    et: 3   # Enhancing tumor

  # Gradient norm logging (optimization stability monitoring)
  log_grad_norm: true
