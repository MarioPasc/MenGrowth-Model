data:
  root_dir: "/media/mpascual/PortableSSD/Meningiomas/BraTS/BraTS_Men_Train"
  modalities: ["t1c", "t1n", "t2f", "t2w"]
  roi_size: [128, 128, 128]
  spacing: [1.875, 1.875, 1.875]
  orientation: "RAS"
  batch_size: 8
  num_workers: 0
  cache_rate: 1.0
  val_split: 0.1
  persistent_cache_subdir: "cache"

model:
  variant: "dipvae"
  z_dim: 128
  input_channels: 4
  base_filters: 32
  norm: "GROUP"  # Normalization type (now properly used via model_factory)

  # Architecture customizations
  architecture:
    pre_activation: false             # Use pre-activation ResNet blocks (NORM->ACT->CONV)
    blocks_per_layer: [2, 2, 2, 2]    # Number of blocks in each of the 4 encoder stages
    upsample_mode: "resize_conv"      # Decoder upsampling: "resize_conv" or "deconv"

  # Spatial Broadcast Decoder (SBD)
  use_sbd: true                     # RECOMMENDED: false = standard decoder, true = SBD
                                     # Start with false to verify encoder learns informative latents
                                     # If collapse persists, architectural changes won't help
  # SBD-specific parameters (only used if use_sbd=true)
  sbd_grid_size: [8, 8, 8]
  sbd_upsample_mode: "resize_conv"  # "resize_conv" prevents checkerboard artifacts



loss:
  # === DIP-VAE-II Covariance Penalties ===
  # Loss = MSE_mean + beta*KL + lambda_od*||Cov_offdiag||_F² + lambda_d*||diag(Cov)-1||_2²
  #
  # IMPORTANT: With reduction="mean", regularization terms (KL + Cov) will dominate
  # the loss magnitude (~96% with default lambdas). This is INTENTIONAL because:
  # 1. Free bits ensures minimum information encoding (prevents KL collapse)
  # 2. Strong covariance penalties enforce disentanglement
  # 3. Delayed start allows reconstruction to stabilize first
  #
  # See tests/test_loss_reduction_analysis.py for detailed analysis.

  lambda_od: 10.0                      # Off-diagonal covariance penalty (DIP-VAE-II)
                                       # Higher = stronger decorrelation between latent dims
  lambda_d: 5.0                        # Diagonal covariance penalty (DIP-VAE-II)
                                       # Higher = stronger push toward unit variance
  lambda_start_epoch: 20               # CRITICAL: Pre-train VAE for 20 epochs before DIP
                                       # Allows encoder to learn informative latents first
                                       # Without this, covariance on random latents is meaningless
  lambda_cov_annealing_epochs: 40      # Linear warmup after start_epoch (prevents optimizer shock)
                                       # Full lambda reached at epoch 20 + 40 = 60
  compute_in_fp32: true                # Compute covariance in FP32 for numerical stability

  # === Loss Reduction Strategy ===
  # We use "mean" reduction for MSE but do NOT scale KL/covariance by 1/N.
  # This creates apparent imbalance (regularization >> recon in magnitude) but is correct:
  # - Sum reduction would give MSE ~16M, causing FP16 overflow
  # - Mean reduction gives MSE ~1, numerically stable for FP16
  # - Keeping regularization unscaled ensures free bits and covariance penalties are effective
  # - The imbalance is handled by delayed start, annealing, and free bits floor
  reduction: "mean"

  # Multi-GPU DDP: all-gather μ,logvar across ranks for larger effective covariance batch
  # Requires drop_last=True for equal batch sizes (enforced in datasets.py when devices > 1)
  use_ddp_gather: true

train:
  seed: 42
  max_epochs: 1000
  lr: 1e-4
  weight_decay: 0.01  # AdamW weight decay (L2 regularization)
  precision: "16-mixed"
  gradient_checkpointing: true
  accelerator: "gpu"
  devices: 1                           # If devices > 1, validation uses drop_last=True for DDP safety

  # Numerical stability settings
  loss_reduction: "mean"
  gradient_clip_val: 5.0               # CHANGED: 0.0 -> 5.0 (enable clipping)
  gradient_clip_algorithm: "norm"

  # Posterior variance floor (prevents numerical underflow)
  posterior_logvar_min: -6.0           # NEW: exp(-6) ≈ 0.0025 variance

  # === KL Regularization - Posterior Collapse Mitigation ===
  # Default strategy: Cyclical Annealing + Free Bits (recommended)

  # Beta annealing (controls KL weight schedule)
  kl_beta: 1.0
  kl_annealing_epochs: 160  # For cyclical: 4 cycles × 40 epochs/cycle
  kl_annealing_type: "cyclical"  # Options: "linear", "cyclical"
  kl_annealing_cycles: 4
  kl_annealing_ratio: 0.5  # Anneal for first 50% of each cycle

  # Free bits (per-dimension KL threshold for posterior collapse mitigation)
  # CRITICAL: With reduction="mean", free bits floor (25.6) >> MSE_mean (~1).
  # This ensures the model MUST encode information to beat the floor.
  # Without free bits, KL would collapse to near-zero (posterior = prior).
  kl_free_bits: 0.2                     # 0.2 nats/dim provides strong collapse prevention
                                       # With batch_mean mode and z_dim=128: floor ≈ 128 × 0.2 = 25.6 nats
                                       # Reduce to 0.1 if KL is too high and hurting reconstruction
  kl_free_bits_mode: "batch_mean"      # "per_sample" or "batch_mean"
                                       # batch_mean: weaker, better for small batches (B=8)
                                       # per_sample: stronger, uniform floor across samples

  # Collapse diagnostics (deterministic recon, z=0 ablation, μ variance)
  log_collapse_diagnostics: true       # Log diag/recon_mu_mse, diag/recon_z0_mse, etc.

logging:
  save_dir: "experiments/runs"

  # Logger configuration
  log_every_n_steps: 50  # How often to log step metrics

  logger:
    type: "wandb"  # "wandb" or "csv" (for backward compat)
    wandb:
      project: "mengrowth-dipvae"
      entity: "mario-pg02-icai"  # Set to your wandb username/team (or use WANDB_ENTITY env var)
      name: null  # Auto-generated: "{experiment}_{timestamp}"
      tags: ["dipvae", "meningioma", "disentanglement", "exp2", "sbd"]
      notes: "Training DIP-VAE with SBD on cluster - Covariance regularization for disentanglement"
      offline: true  # CRITICAL for cluster nodes without internet
      save_code: true  # Save code snapshot to wandb
      log_model: false  # We handle checkpoints manually

  # Checkpointing
  checkpointing:
    save_top_k: 3
    monitor: "val_epoch/loss"
    mode: "min"
    save_last: true
    every_n_epochs: 1
    filename: "dipvae-{epoch:04d}"  # Note: {val_loss} removed (key is val_epoch/loss)

  # Visual logging
  visual:
    log_reconstructions: true
    recon_every_n_epochs: 5
    num_recon_samples: 2
    log_dashboard: true
    dashboard_every_n_epochs: 10
    log_latent_viz: true
    latent_viz_every_n_epochs: 20
    latent_viz_n_samples: 512

  # Legacy settings (keep for backward compat)
  min_logs_per_epoch: 3                # Minimum console/CSV log entries per training epoch

  # Latent diagnostics (lightweight disentanglement metrics)
  latent_diag_every_n_epochs: 10
  latent_diag_num_samples: 128         # Increased from 32 for better covariance estimation
                                       # 128-512 recommended for z_dim=128
  latent_diag_shift_vox: 5
  latent_diag_csv_name: "latent_diag/metrics.csv"
  latent_diag_ids_name: "latent_diag/ids.txt"

  # Active Units (AU) callback - canonical latent activity metric
  au_dense_until: 30                   # Dense logging (every epoch) for epochs 0..30
  au_sparse_interval: 5                # Sparse logging (every 5 epochs) after epoch 30
  au_subset_fraction: 0.5              # Use 50% of validation dataset for AU computation (0.99 was inefficient)
  au_batch_size: 64                    # Batch size for AU computation (larger = more efficient)
  eps_au: 0.01                         # Variance threshold (nats) for active dimension counting
  au_subset_seed: 42                   # RNG seed for reproducible subset selection

  # Tidy CSV logging (one row per epoch, no NaN fragmentation)
  tidy_dir: "logs/tidy"
  step_log_stride: 50                 # Log every N steps for step CSV (50-200 recommended for long runs)

  # Segmentation label mapping (configurable for different datasets)
  seg_labels:
    ncr: 1  # Necrotic core
    ed: 2   # Edema
    et: 3   # Enhancing tumor

  # Gradient norm logging (optimization stability monitoring)
  log_grad_norm: true
