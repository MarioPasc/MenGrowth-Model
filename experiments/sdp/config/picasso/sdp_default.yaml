# experiments/sdp/config/picasso/sdp_default.yaml
# =============================================================================
# SDP Experiment Configuration — Picasso A100 80GB
# =============================================================================
# Hardware-optimized for Picasso: A100 80GB GPU, 16 CPUs, 64G RAM.
# Uses H5 backend for fast I/O (single file vs 5000 NIfTIs).

# =============================================================================
# Paths (Picasso)
# =============================================================================
paths:
  checkpoint_dir: /mnt/home/users/tic_163_uma/mpascual/fscratch/checkpoints/BrainSegFounder_finetuned_BraTS
  lora_checkpoint: /mnt/home/users/tic_163_uma/mpascual/execs/growth/results/lora_ablation_semantic_heads/conditions/lora_r8
  data_root: /mnt/home/users/tic_163_uma/mpascual/fscratch/datasets/meningiomas/brats_men
  h5_file: /mnt/home/users/tic_163_uma/mpascual/fscratch/datasets/meningiomas/brats_men_train.h5
  output_dir: /mnt/home/users/tic_163_uma/mpascual/execs/growth/results/SDP_Module/runs/sdp_default
  features_dir: ${paths.output_dir}/features

# =============================================================================
# Data Splits
# =============================================================================
data:
  train_splits: [lora_train, sdp_train]  # Combined for SDP training
  val_split: lora_val                     # Validation
  test_split: test                        # Held-out evaluation
  splits_config: experiments/lora_ablation/config/picasso/LoRA_semantic_heads_icai.yaml

# =============================================================================
# SDP Architecture (from phase2_sdp.yaml)
# =============================================================================
sdp:
  in_dim: 768
  hidden_dim: 512
  out_dim: 128
  dropout: 0.1
  spectral_norm: all

partition:
  vol_dim: 24
  loc_dim: 8
  shape_dim: 12
  residual_dim: 84

targets:
  n_vol: 4
  n_loc: 3
  n_shape: 3

# =============================================================================
# Loss
# =============================================================================
loss:
  lambda_vol: 20.0
  lambda_loc: 12.0
  lambda_shape: 15.0
  lambda_cov: 5.0
  lambda_var: 5.0
  lambda_dcor: 2.0
  gamma_var: 1.0

# =============================================================================
# Curriculum
# =============================================================================
curriculum:
  enabled: true
  warmup_end: 10
  semantic_end: 40
  independence_end: 60

# =============================================================================
# Training
# =============================================================================
training:
  seed: 42
  max_epochs: 100
  lr: 1.0e-3
  weight_decay: 0.01
  batch_size: full
  optimizer: adamw
  gradient_clip_val: 1.0
  scheduler:
    type: cosine
    warmup_epochs: 5
    min_lr: 1.0e-6
  precision: 32-true
  accelerator: auto
  devices: 1

# =============================================================================
# Normalization
# =============================================================================
normalization:
  scope: train_only
  features: true
  targets: true

# =============================================================================
# Feature Extraction — A100 optimized
# =============================================================================
feature_extraction:
  level: encoder10
  batch_size: 4              # A100 80GB fits 192³×4 at eval+bf16
  num_workers: 16            # 128 cores / 8 GPUs per node
  roi_size: [192, 192, 192]

# =============================================================================
# Logging
# =============================================================================
logging:
  log_every_n_steps: 1
