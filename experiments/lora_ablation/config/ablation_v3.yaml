# experiments/lora_ablation/config/ablation_v3.yaml
# LoRA Phase v3: Feature-Rich Encoder Adaptation
#
# Key changes from v2:
# - VICReg encoder loss (lambda_var_enc, lambda_cov_enc) to fight dimensional collapse
# - Boosted lambda_aux (0.1 -> 0.3) for stronger semantic supervision
# - Reduced lambda_location (1.0 -> 0.3) due to architectural limitation
# - Revised shape targets: [sphericity, enhancement_ratio, infiltration_index]
# - Configurable target_stages per condition
# - Longer warmup (15 epochs) and more epochs (150)
# - Mixed precision + gradient accumulation for better batch statistics

experiment:
  name: lora_ablation_v3
  seed: 42
  output_dir: /media/hddb/mario/results/growth/lora_ablation_v3

paths:
  checkpoint: /media/hddb/mario/data/BrainSegFounder_finetuned_BraTS/finetuned_model_fold_0.pt
  data_root: /media/hddb/mario/data/BraTS-MEN
  glioma_root: /media/hddb/mario/data/BraTS-GLI-100

# Spatial configuration
data:
  roi_size: [128, 128, 128]
  feature_roi_size: [192, 192, 192]
  spacing: [1.0, 1.0, 1.0]

# Production-aligned splits (matches picasso/v3_rank_sweep.yaml)
data_splits:
  lora_train: 525
  lora_val: 100
  sdp_train: 225
  test: 150

# =============================================================================
# Ablation Conditions (10 total)
# =============================================================================
conditions:
  # Reference: no training
  - name: baseline_frozen
    lora_rank: null
    skip_training: true
    description: Original BrainSegFounder (completely frozen)

  # Reference: decoder-only fine-tuning
  - name: baseline
    lora_rank: null
    description: Frozen encoder + trainable decoder (no adaptation)

  # v2 reproduction: stages 3-4, original lambda_aux, no VICReg
  - name: lora_r16_vanilla
    lora_rank: 16
    lora_alpha: 32
    description: LoRA r16 stages 3-4, lambda_aux=0.1, no VICReg (v2 reproduction)

  # + VICReg only
  - name: lora_r16_vicreg
    lora_rank: 16
    lora_alpha: 32
    use_vicreg: true
    description: LoRA r16 + VICReg (variance + covariance regularization)

  # + Boosted aux only
  - name: lora_r16_boosted
    lora_rank: 16
    lora_alpha: 32
    lambda_aux_override: 0.3
    description: LoRA r16 + boosted lambda_aux=0.3

  # + Both VICReg + boosted aux (recommended)
  - name: lora_r16_full
    lora_rank: 16
    lora_alpha: 32
    use_vicreg: true
    lambda_aux_override: 0.3
    description: LoRA r16 + VICReg + lambda_aux=0.3 (full v3)

  # Higher rank baseline
  - name: lora_r32_vanilla
    lora_rank: 32
    lora_alpha: 64
    description: LoRA r32 stages 3-4, lambda_aux=0.1, no VICReg

  # Higher rank + full treatment
  - name: lora_r32_full
    lora_rank: 32
    lora_alpha: 64
    use_vicreg: true
    lambda_aux_override: 0.3
    description: LoRA r32 + VICReg + lambda_aux=0.3

  # Extended to stages 2-4 (includes 192-dim stage)
  - name: lora_r16_stages234
    lora_rank: 16
    lora_alpha: 32
    target_stages: [2, 3, 4]
    use_vicreg: true
    lambda_aux_override: 0.3
    description: LoRA r16 stages 2-4 + VICReg + lambda_aux=0.3

  # Higher rank + extended stages
  - name: lora_r32_stages234
    lora_rank: 32
    lora_alpha: 64
    target_stages: [2, 3, 4]
    use_vicreg: true
    lambda_aux_override: 0.3
    description: LoRA r32 stages 2-4 + VICReg + lambda_aux=0.3

# =============================================================================
# Training Hyperparameters
# =============================================================================
training:
  max_epochs: 150
  early_stopping_patience: 30
  batch_size: 4
  lr_encoder: 1.0e-4
  lr_decoder: 5.0e-4
  weight_decay: 1.0e-5
  num_workers: 4
  lora_dropout: 0.1
  gradient_clip: 1.0

  # Decoder configuration
  decoder_type: "original"
  freeze_decoder: false
  use_semantic_heads: true

  # Auxiliary loss (base value — conditions can override via lambda_aux_override)
  lambda_aux: 0.1

  # Auxiliary loss warmup
  aux_warmup_epochs: 5
  aux_warmup_duration: 15

  # VICReg encoder regularization (only active for conditions with use_vicreg: true)
  # These are per-condition overridable via the condition config.
  # Activated by setting lambda_var_enc > 0 and/or lambda_cov_enc > 0.
  lambda_var_enc: 5.0
  lambda_cov_enc: 1.0
  vicreg_gamma: 1.0

  # LR schedule: warmup → ReduceLROnPlateau
  lr_warmup_epochs: 5
  lr_reduce_factor: 0.5
  lr_reduce_patience: 10

  # Mixed precision and gradient accumulation
  use_amp: true
  grad_accum_steps: 2

  # Gradient monitoring
  enable_gradient_monitoring: true
  gradient_monitor_freq: 50

# =============================================================================
# Loss Weights
# =============================================================================
loss:
  lambda_dice: 1.0
  lambda_ce: 1.0
  # Auxiliary semantic loss component weights
  lambda_volume: 1.0
  lambda_location: 0.3      # Reduced from 1.0 (architectural limitation)
  lambda_shape: 1.0

# =============================================================================
# Probe Evaluation
# =============================================================================
probe:
  alpha_linear: 1.0
  use_mlp_probes: true
  alpha_mlp: 0.01
  hidden_sizes: [128]
  normalize_features: true
  normalize_targets: true

# =============================================================================
# Feature Extraction
# =============================================================================
feature_extraction:
  level: encoder10
  batch_size: 2
  # Pooling mode: "gap" (default), "tap" (tumor-aware), "both"
  pooling_mode: both

# =============================================================================
# Domain Analysis
# =============================================================================
domain_analysis:
  enabled: true
  num_samples: 200

# =============================================================================
# Visualization
# =============================================================================
visualization:
  umap_n_neighbors: 15
  umap_min_dist: 0.1
  color_by_semantic: true
  show_variance_per_dim: true
  show_scatter_plots: true

# Logging
logging:
  log_every_n_steps: 10
  val_check_interval: 1.0

# Model card generation
model_card:
  enabled: true
