# experiments/lora_ablation/config/server/semantic_heads_icai.yaml
# LoRA Ablation Experiment Configuration - WITH Semantic Heads
#
# This config enables auxiliary semantic prediction losses during training.
# The semantic heads predict volume, location, and shape from bottleneck features.

experiment:
  name: lora_ablation_semantic_heads
  seed: 42
  output_dir: /media/mpascual/Sandisk2TB/research/growth-dynamics/growth/results/BrainSegFounder_adaptation/LoRA_Adaptation/lora_ablation_semantic_heads

paths:
  checkpoint: /media/mpascual/Sandisk2TB/research/growth-dynamics/growth/checkpoints/BrainSegFounder_finetuned_BraTS/finetuned_model_fold_0.pt
  data_root: /media/mpascual/PortableSSD/Meningiomas/BraTS/BraTS_Men_Train
  # Optional: glioma data for domain shift analysis (required for --domain-features)
  glioma_root: /media/mpascual/PortableSSD/BraTS_GLI/BraTS-GLI-100

# Updated splits (300/50/150/500)
data_splits:
  lora_train: 525
  lora_val: 100
  sdp_train: 225
  test: 150

# Experimental conditions
# NOTE: Using 3-channel output with sigmoid activation preserves the FULL pretrained
# decoder weights from BrainSegFounder, making baseline_frozen evaluation valid.
conditions:
  - name: baseline_frozen
    lora_rank: null
    skip_training: true
    description: Original BrainSegFounder (completely frozen, test only)

  - name: baseline
    lora_rank: null
    description: Frozen encoder + trainable decoder (no adaptation)

  - name: lora_r2
    lora_rank: 2
    lora_alpha: 4
    description: LoRA rank 2

  - name: lora_r4
    lora_rank: 4
    lora_alpha: 8
    description: LoRA rank 4

  - name: lora_r8
    lora_rank: 8
    lora_alpha: 16
    description: LoRA rank 8 (recommended)

  - name: lora_r16
    lora_rank: 16
    lora_alpha: 32
    description: LoRA rank 16

  - name: lora_r32
    lora_rank: 32
    lora_alpha: 64
    description: LoRA rank 32 (saturation test)

# Training hyperparameters
training:
  max_epochs: 100
  early_stopping_patience: 25  # Increased patience for stable training
  batch_size: 2            # Reduced from 4 (128³ ROI requires ~2.4x more memory than 96³)
  lr_encoder: 1.0e-4      # For LoRA params
  lr_decoder: 5.0e-4      # For decoder (if trainable)
  weight_decay: 1.0e-5
  num_workers: 4
  lora_dropout: 0.1
  gradient_clip: 1.0

  # Decoder configuration
  decoder_type: "original"   # "lightweight" | "original"
  freeze_decoder: false      # Train decoder (for better gradients)

  # Semantic heads configuration
  use_semantic_heads: true   # Enable auxiliary semantic prediction
  lambda_aux: 0.1            # Weight for auxiliary loss

  # Auxiliary loss warmup (prevents destabilizing early training)
  # - aux_warmup_epochs: Epoch to START auxiliary loss (0 = immediate)
  # - aux_warmup_duration: Epochs to ramp lambda_aux from 0 to target
  aux_warmup_epochs: 5       # Start aux loss at epoch 5
  aux_warmup_duration: 10    # Ramp over 10 epochs (full at epoch 15)

  # Gradient monitoring (optional, for diagnostics)
  enable_gradient_monitoring: true   # Log gradient norms per group
  gradient_monitor_freq: 50          # Log every N batches

# Segmentation loss
loss:
  lambda_dice: 1.0
  lambda_ce: 1.0

  # Auxiliary semantic loss weights (per-feature)
  # Note: Shape is harder to predict (R² ~0.25), so reduced weight
  lambda_volume: 1.0         # Volume prediction weight
  lambda_location: 1.0       # Location prediction weight
  lambda_shape: 0.5          # Shape prediction weight (reduced for harder task)

# Probe evaluation (simplified MLP)
probe:
  alpha_linear: 1.0           # Ridge regularization
  use_mlp_probes: true        # Enable MLP probes
  alpha_mlp: 0.01             # MLP L2 regularization (increased to prevent overfitting)
  hidden_sizes: [128]         # Simplified from [256, 128]
  normalize_features: true
  normalize_targets: true     # Normalize targets for stable training

# Feature extraction
feature_extraction:
  level: encoder10            # 768-dim (matches SDP Phase 2)
  batch_size: 1              # Reduced from 4 to avoid OOM on local GPU (128³ ROI)

# Domain analysis (glioma vs meningioma)
domain_analysis:
  enabled: true
  num_samples: 200           # Samples per domain for UMAP

# Visualization
visualization:
  umap_n_neighbors: 15
  umap_min_dist: 0.1
  color_by_semantic: true    # Color UMAP by semantic features
  show_variance_per_dim: true
  show_scatter_plots: true   # Predictions vs ground truth

# Logging
logging:
  log_every_n_steps: 10
  val_check_interval: 1.0

# Model card generation
model_card:
  enabled: true

# =============================================================================
# METHODOLOGICAL NOTES
# =============================================================================
#
# This configuration implements several best practices for multi-task learning:
#
# 1. TARGET NORMALIZATION
#    Semantic targets are normalized to zero mean, unit variance to prevent
#    scale mismatch between loss components (Kendall et al., 2018).
#
# 2. AUXILIARY LOSS WARMUP
#    The auxiliary loss starts at epoch 5 and ramps over 10 epochs.
#    This allows the encoder to learn basic features before semantic pressure.
#
# 3. PER-FEATURE WEIGHTING
#    Shape features are harder to predict (R² ~0.25 vs ~0.70 for volume).
#    Reduced lambda_shape to 0.5 prevents the harder task from dominating.
#
# 4. GRADIENT MONITORING
#    Gradient norms are logged per parameter group to detect imbalances.
#    Check training_log.csv for encoder/decoder/semantic gradient norms.
#
# 5. EARLY STOPPING ON DICE
#    Early stopping uses validation Dice (primary metric), not total loss.
#    This prevents auxiliary loss improvements from masking segmentation degradation.
#
# References:
# - Kendall et al. (2018): Multi-Task Learning Using Uncertainty to Weigh Losses
# - Chen et al. (2018): GradNorm for gradient balancing
# - Liu et al. (2019): Dynamic Weight Average (DWA)
