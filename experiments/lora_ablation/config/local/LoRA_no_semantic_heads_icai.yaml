# experiments/lora_ablation/config/server/no_semantic_heads_icai.yaml
# LoRA Ablation Experiment Configuration - WITHOUT Semantic Heads
#
# This config disables auxiliary semantic prediction losses.
# Training uses only segmentation loss (Dice + CE).
# This is the control experiment for measuring the effect of semantic heads.

experiment:
  name: lora_ablation_no_semantic_heads
  seed: 42
  output_dir: /media/mpascual/Sandisk2TB/research/growth-dynamics/growth/results/BrainSegFounder_adaptation/LoRA_Adaptation/lora_ablation_no_semantic_heads

paths:
  checkpoint: /media/mpascual/Sandisk2TB/research/growth-dynamics/growth/checkpoints/BrainSegFounder_finetuned_BraTS/finetuned_model_fold_0.pt
  data_root: /media/mpascual/PortableSSD/Meningiomas/BraTS/BraTS_Men_Train
  h5_file: /media/mpascual/Sandisk2TB/research/growth-dynamics/growth/data/brats_men_train.h5
  # Optional: glioma data for domain shift analysis (required for --domain-features)
  glioma_root: /media/mpascual/PortableSSD/BraTS_GLI/BraTS-GLI-100

# Spatial configuration
data:
  roi_size: [128, 128, 128]           # LoRA training (matches BrainSegFounder fine-tuning)
  feature_roi_size: [192, 192, 192]   # Feature extraction & eval (100% tumor containment)
  spacing: [1.0, 1.0, 1.0]           # Isotropic 1mm (BraTS native resolution)

# Updated splits (300/50/150/500)
data_splits:
  lora_train: 525
  lora_val: 100
  sdp_train: 225
  test: 150

# Experimental conditions
# NOTE: Using 3-channel output with sigmoid activation preserves the FULL pretrained
# decoder weights from BrainSegFounder, making baseline_frozen evaluation valid.
conditions:
  - name: baseline_frozen
    lora_rank: null
    skip_training: true
    description: Original BrainSegFounder (completely frozen, test only)

  - name: baseline
    lora_rank: null
    description: Frozen encoder + trainable decoder (no adaptation)

  - name: lora_r2
    lora_rank: 2
    lora_alpha: 4
    description: LoRA rank 2

  - name: lora_r4
    lora_rank: 4
    lora_alpha: 8
    description: LoRA rank 4

  - name: lora_r8
    lora_rank: 8
    lora_alpha: 16
    description: LoRA rank 8 (recommended)

  - name: lora_r16
    lora_rank: 16
    lora_alpha: 32
    description: LoRA rank 16

  - name: lora_r32
    lora_rank: 32
    lora_alpha: 64
    description: LoRA rank 32 (saturation test)

# Training hyperparameters
training:
  max_epochs: 100
  early_stopping_patience: 25  # Increased patience for stable training
  batch_size: 2            # 128³ LoRA training ROI
  lr_encoder: 1.0e-4      # For LoRA params
  lr_decoder: 5.0e-4      # For decoder (if trainable)
  weight_decay: 1.0e-5
  num_workers: 4
  lora_dropout: 0.1
  gradient_clip: 1.0

  # Decoder configuration
  decoder_type: "original"   # "lightweight" | "original"
  freeze_decoder: false      # Train decoder (for better gradients)

  # Semantic heads configuration
  use_semantic_heads: false  # DISABLED - no auxiliary semantic prediction
  lambda_aux: 0.0            # Not used when use_semantic_heads: false

  # Warmup not needed when semantic heads disabled
  aux_warmup_epochs: 0
  aux_warmup_duration: 0

  # Gradient monitoring (for diagnostics - always enabled for analysis)
  enable_gradient_monitoring: true
  gradient_monitor_freq: 50

# Segmentation loss
loss:
  lambda_dice: 1.0
  lambda_ce: 1.0

  # These are not used when use_semantic_heads: false, but kept for reference
  lambda_volume: 1.0
  lambda_location: 1.0
  lambda_shape: 0.5

# Probe evaluation (simplified MLP)
probe:
  alpha_linear: 1.0           # Ridge regularization
  use_mlp_probes: true        # Enable MLP probes
  alpha_mlp: 0.01             # MLP L2 regularization (increased to prevent overfitting)
  hidden_sizes: [128]         # Simplified from [256, 128]
  normalize_features: true
  normalize_targets: true     # Normalize targets for stable training

# Feature extraction
feature_extraction:
  level: encoder10            # 768-dim (matches SDP Phase 2)
  batch_size: 1              # 192³ feature ROI; kept at 1 for local GPU memory

# Domain analysis (glioma vs meningioma)
domain_analysis:
  enabled: true
  num_samples: 200           # Samples per domain for UMAP

# Visualization
visualization:
  umap_n_neighbors: 15
  umap_min_dist: 0.1
  color_by_semantic: true    # Color UMAP by semantic features
  show_variance_per_dim: true
  show_scatter_plots: true   # Predictions vs ground truth

# Logging
logging:
  log_every_n_steps: 10
  val_check_interval: 1.0

# Model card generation
model_card:
  enabled: true

# =============================================================================
# EXPERIMENT NOTES
# =============================================================================
#
# This is the CONTROL experiment without semantic heads.
#
# Compare results with semantic_heads_icai.yaml to measure the effect of
# auxiliary semantic prediction on:
#
# 1. SEGMENTATION QUALITY
#    Does the auxiliary loss improve or hurt Dice scores?
#
# 2. REPRESENTATION QUALITY
#    Are semantic features more linearly decodable with auxiliary supervision?
#    Compare linear probe R² between experiments.
#
# 3. NONLINEARITY GAP
#    Does auxiliary supervision reduce the gap between linear and MLP probes?
#    A smaller gap suggests more linearly organized representations.
#
# The key question: Does "teaching to the test" (auxiliary semantic loss)
# improve or bias the learned representations?
