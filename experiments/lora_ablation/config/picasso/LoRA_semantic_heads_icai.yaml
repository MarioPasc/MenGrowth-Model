# experiments/lora_ablation/config/picasso/LoRA_semantic_heads_icai.yaml
# LoRA Ablation — WITH Semantic Heads — Picasso A100 80GB
#
# Hardware-optimized for Picasso: 4 nodes × 8 A100 80GB GPUs, 128 cores/node.
# Each experiment uses 1 GPU. Key differences from server config:
#   - batch_size: 2 → 4 (bf16 halves memory; A100 80GB fits 128³×4)
#   - max_epochs: 100 → 200 (2× fewer steps/epoch → 2× epochs)
#   - patience: 25 → 40 (proportional to max_epochs)
#   - num_workers: 4 → 16 (128 cores / 8 GPUs per node)
#   - use_amp: true (native bf16 tensor cores on A100)
#   - feature_extraction.batch_size: 2 → 4 (192³×4 fits in 80GB at eval+bf16)
#   - aux warmup scaled: 5→10 start, 10→20 duration

experiment:
  name: lora_ablation_semantic_heads
  seed: 42
  output_dir: /mnt/home/users/tic_163_uma/mpascual/execs/growth/results/lora_ablation_semantic_heads

paths:
  checkpoint: /mnt/home/users/tic_163_uma/mpascual/fscratch/checkpoints/BrainSegFounder_finetuned_BraTS/finetuned_model_fold_0.pt
  data_root: /mnt/home/users/tic_163_uma/mpascual/fscratch/datasets/meningiomas/brats_men
  glioma_root: /mnt/home/users/tic_163_uma/mpascual/fscratch/datasets/meningiomas/glioma/high_resolution

# Spatial configuration
data:
  roi_size: [128, 128, 128]           # LoRA training (matches BrainSegFounder fine-tuning)
  feature_roi_size: [192, 192, 192]   # Feature extraction & eval (100% tumor containment)
  spacing: [1.0, 1.0, 1.0]           # Isotropic 1mm (BraTS native resolution)

data_splits:
  lora_train: 525
  lora_val: 100
  sdp_train: 225
  test: 150

# Experimental conditions
conditions:
  - name: baseline_frozen
    lora_rank: null
    skip_training: true
    description: Original BrainSegFounder (completely frozen, test only)

  - name: baseline
    lora_rank: null
    description: Frozen encoder + trainable decoder (no adaptation)

  - name: lora_r2
    lora_rank: 2
    lora_alpha: 4
    description: LoRA rank 2

  - name: lora_r4
    lora_rank: 4
    lora_alpha: 8
    description: LoRA rank 4

  - name: lora_r8
    lora_rank: 8
    lora_alpha: 16
    description: LoRA rank 8 (recommended)

  - name: lora_r16
    lora_rank: 16
    lora_alpha: 32
    description: LoRA rank 16

  - name: lora_r32
    lora_rank: 32
    lora_alpha: 64
    description: LoRA rank 32 (saturation test)

# Training hyperparameters — A100 optimized
training:
  max_epochs: 200
  early_stopping_patience: 40
  batch_size: 4              # 2× server (bf16 halves memory on A100 80GB)
  lr_encoder: 1.0e-4         # Kept same; larger batch → better gradient estimates
  lr_decoder: 5.0e-4
  weight_decay: 1.0e-5
  num_workers: 16            # 128 cores / 8 GPUs per node
  lora_dropout: 0.1
  gradient_clip: 1.0

  # Mixed precision (A100 native bf16 tensor cores)
  use_amp: true              # bf16 autocast — no GradScaler needed
  grad_accum_steps: 1        # batch_size=4 is sufficient

  # Decoder configuration
  decoder_type: "original"
  freeze_decoder: false

  # Semantic heads configuration
  use_semantic_heads: true
  lambda_aux: 0.1

  # Auxiliary loss warmup (scaled for 2× epochs)
  aux_warmup_epochs: 10      # 2× server (5 → 10)
  aux_warmup_duration: 20    # 2× server (10 → 20)

  # Gradient monitoring
  enable_gradient_monitoring: true
  gradient_monitor_freq: 50

# Segmentation loss
loss:
  lambda_dice: 1.0
  lambda_ce: 1.0
  lambda_volume: 1.0
  lambda_location: 1.0
  lambda_shape: 0.5

# Probe evaluation
probe:
  alpha_linear: 1.0
  use_mlp_probes: true
  alpha_mlp: 0.01
  hidden_sizes: [128]
  normalize_features: true
  normalize_targets: true

# Feature extraction — A100 optimized
feature_extraction:
  level: encoder10
  batch_size: 4              # 2× server (192³×4 fits in 80GB at eval+bf16)

# Domain analysis
domain_analysis:
  enabled: true
  num_samples: 200

# Visualization
visualization:
  umap_n_neighbors: 15
  umap_min_dist: 0.1
  color_by_semantic: true
  show_variance_per_dim: true
  show_scatter_plots: true

# Logging
logging:
  log_every_n_steps: 10
  val_check_interval: 1.0

# Model card generation
model_card:
  enabled: true
