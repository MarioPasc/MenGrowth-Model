# experiments/lora_ablation/config/picasso/DoRA_semantic_heads_icai.yaml
# DoRA Ablation — WITH Semantic Heads — Picasso A100 80GB
#
# DoRA (Weight-Decomposed LoRA) with auxiliary semantic prediction.
# Hardware-optimized for Picasso A100 80GB (see LoRA_semantic_heads for rationale).

experiment:
  name: dora_ablation_semantic_heads
  seed: 42
  output_dir: /mnt/home/users/tic_163_uma/mpascual/execs/growth/results/dora_ablation_semantic_heads

paths:
  checkpoint: /mnt/home/users/tic_163_uma/mpascual/fscratch/checkpoints/BrainSegFounder_finetuned_BraTS/finetuned_model_fold_0.pt
  data_root: /mnt/home/users/tic_163_uma/mpascual/fscratch/datasets/meningiomas/brats_men
  glioma_root: /mnt/home/users/tic_163_uma/mpascual/fscratch/datasets/meningiomas/glioma/high_resolution

# Spatial configuration
data:
  roi_size: [128, 128, 128]
  feature_roi_size: [192, 192, 192]
  spacing: [1.0, 1.0, 1.0]

data_splits:
  lora_train: 525
  lora_val: 100
  sdp_train: 225
  test: 150

# Experimental conditions — DoRA only
conditions:
  - name: baseline_frozen
    lora_rank: null
    skip_training: true
    description: Original BrainSegFounder (completely frozen, test only)

  - name: baseline
    lora_rank: null
    description: Frozen encoder + trainable decoder (no adaptation)

  - name: dora_r2
    lora_rank: 2
    lora_alpha: 4
    use_dora: true
    description: DoRA rank 2 (weight-decomposed)

  - name: dora_r4
    lora_rank: 4
    lora_alpha: 8
    use_dora: true
    description: DoRA rank 4 (weight-decomposed)

  - name: dora_r8
    lora_rank: 8
    lora_alpha: 16
    use_dora: true
    description: DoRA rank 8 (weight-decomposed, recommended)

  - name: dora_r16
    lora_rank: 16
    lora_alpha: 32
    use_dora: true
    description: DoRA rank 16 (weight-decomposed)

  - name: dora_r32
    lora_rank: 32
    lora_alpha: 64
    use_dora: true
    description: DoRA rank 32 (weight-decomposed, saturation test)

# Training hyperparameters — A100 optimized
training:
  max_epochs: 200
  early_stopping_patience: 40
  batch_size: 4
  lr_encoder: 1.0e-4
  lr_decoder: 5.0e-4
  weight_decay: 1.0e-5
  num_workers: 16
  lora_dropout: 0.1
  gradient_clip: 1.0

  # Mixed precision (A100 native bf16 tensor cores)
  use_amp: true
  grad_accum_steps: 1

  # Decoder configuration
  decoder_type: "original"
  freeze_decoder: false

  # Semantic heads configuration
  use_semantic_heads: true
  lambda_aux: 0.1

  # Auxiliary loss warmup (scaled for 2× epochs)
  aux_warmup_epochs: 10
  aux_warmup_duration: 20

  # Gradient monitoring
  enable_gradient_monitoring: true
  gradient_monitor_freq: 50

# Segmentation loss
loss:
  lambda_dice: 1.0
  lambda_ce: 1.0
  lambda_volume: 1.0
  lambda_location: 1.0
  lambda_shape: 0.5

# Probe evaluation
probe:
  alpha_linear: 1.0
  use_mlp_probes: true
  alpha_mlp: 0.01
  hidden_sizes: [128]
  normalize_features: true
  normalize_targets: true

# Feature extraction — A100 optimized
feature_extraction:
  level: encoder10
  batch_size: 4

# Domain analysis
domain_analysis:
  enabled: true
  num_samples: 200

# Visualization
visualization:
  umap_n_neighbors: 15
  umap_min_dist: 0.1
  color_by_semantic: true
  show_variance_per_dim: true
  show_scatter_plots: true

# Logging
logging:
  log_every_n_steps: 10
  val_check_interval: 1.0

# Model card generation
model_card:
  enabled: true
