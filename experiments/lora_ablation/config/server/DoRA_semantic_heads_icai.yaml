# experiments/lora_ablation/config/server/dora_semantic_heads_icai.yaml
# DoRA Ablation Experiment Configuration - WITH Semantic Heads
#
# DoRA (Weight-Decomposed LoRA) decomposes weights into magnitude and direction,
# often providing better performance than standard LoRA.
#
# This config enables auxiliary semantic prediction losses during training.

experiment:
  name: dora_ablation_semantic_heads
  seed: 42
  output_dir: /media/hddb/mario/results/growth/dora_ablation_semantic_heads

paths:
  checkpoint: /media/hddb/mario/data/BrainSegFounder_finetuned_BraTS/finetuned_model_fold_0.pt
  data_root: /media/hddb/mario/data/BraTS-MEN
  glioma_root: /media/hddb/mario/data/BraTS-GLI-100

# Updated splits (300/50/150/500)
data_splits:
  lora_train: 250
  lora_val: 50
  probe_train: 200
  test: 500

# Experimental conditions - DoRA only
# NOTE: Using 3-channel output with sigmoid activation preserves the FULL pretrained
# decoder weights from BrainSegFounder, making baseline_frozen evaluation valid.
conditions:
  - name: baseline_frozen
    lora_rank: null
    skip_training: true
    description: Original BrainSegFounder (completely frozen, test only)

  - name: baseline
    lora_rank: null
    description: Frozen encoder + trainable decoder (no adaptation)

  - name: dora_r2
    lora_rank: 2
    lora_alpha: 4
    use_dora: true
    description: DoRA rank 2 (weight-decomposed)

  - name: dora_r4
    lora_rank: 4
    lora_alpha: 8
    use_dora: true
    description: DoRA rank 4 (weight-decomposed)

  - name: dora_r8
    lora_rank: 8
    lora_alpha: 16
    use_dora: true
    description: DoRA rank 8 (weight-decomposed, recommended)

  - name: dora_r16
    lora_rank: 16
    lora_alpha: 32
    use_dora: true
    description: DoRA rank 16 (weight-decomposed)

  - name: dora_r32
    lora_rank: 32
    lora_alpha: 64
    use_dora: true
    description: DoRA rank 32 (weight-decomposed, saturation test)

# Training hyperparameters
training:
  max_epochs: 100
  early_stopping_patience: 25
  batch_size: 2            # Reduced from 4 (128³ ROI requires ~2.4x more memory than 96³)
  lr_encoder: 1.0e-4
  lr_decoder: 5.0e-4
  weight_decay: 1.0e-5
  num_workers: 4
  lora_dropout: 0.1
  gradient_clip: 1.0

  # Decoder configuration
  decoder_type: "original"
  freeze_decoder: false

  # Semantic heads configuration
  use_semantic_heads: true
  lambda_aux: 0.1

  # Auxiliary loss warmup
  aux_warmup_epochs: 5
  aux_warmup_duration: 10

  # Gradient monitoring
  enable_gradient_monitoring: true
  gradient_monitor_freq: 50

# Segmentation loss
loss:
  lambda_dice: 1.0
  lambda_ce: 1.0
  lambda_volume: 1.0
  lambda_location: 1.0
  lambda_shape: 0.5

# Probe evaluation
probe:
  alpha_linear: 1.0
  use_mlp_probes: true
  alpha_mlp: 0.01
  hidden_sizes: [128]
  normalize_features: true
  normalize_targets: true

# Feature extraction
feature_extraction:
  level: encoder10
  batch_size: 4              # Reduced from 8 (128³ ROI)

# Domain analysis
domain_analysis:
  enabled: true
  num_samples: 200

# Visualization
visualization:
  umap_n_neighbors: 15
  umap_min_dist: 0.1
  color_by_semantic: true
  show_variance_per_dim: true
  show_scatter_plots: true

# Logging
logging:
  log_every_n_steps: 10
  val_check_interval: 1.0

# Model card generation
model_card:
  enabled: true

# =============================================================================
# EXPERIMENT NOTES
# =============================================================================
#
# DoRA (Weight-Decomposed Low-Rank Adaptation) separates the weight matrix into
# magnitude and direction components, allowing for more effective adaptation.
#
# Reference: Liu et al. (2024). "DoRA: Weight-Decomposed Low-Rank Adaptation."
# arXiv:2402.09353
#
# Compare with LoRA_semantic_heads_icai.yaml to assess DoRA benefits.
